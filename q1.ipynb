{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHPsK7dTJABkSUXti4W91F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Quantamaster/ViT-SAM2-Vision-Lab-Pro-IISc-/blob/main/q1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTc1tzQbMUWO",
        "outputId": "49594484-50a1-4d90-e238-ecd83f2d4139"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/123.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m122.9/123.5 kB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hPyTorch 2.8.0+cu126 Device: CPU\n"
          ]
        }
      ],
      "source": [
        "!pip install -q timm albumentations==1.3.0\n",
        "\n",
        "# Use GPU\n",
        "import torch\n",
        "print(\"PyTorch\", torch.__version__, \"Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: imports\n",
        "import math, time, os, random\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "import numpy as np\n",
        "\n",
        "# for progress\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "7Wbp3EgBMYFg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: config\n",
        "class CFG:\n",
        "    seed = 42\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    data_dir = \"/content/data\"\n",
        "    batch_size = 256                   # use 512 if GPU memory allows\n",
        "    epochs = 200\n",
        "    lr = 3e-4\n",
        "    weight_decay = 0.05\n",
        "    img_size = 32\n",
        "    patch_size = 4                     # 4 -> (8x8) patches => 8*8=64 patches\n",
        "    in_chans = 3\n",
        "    num_classes = 10\n",
        "    embed_dim = 192                    # small/tiny model\n",
        "    depth = 12\n",
        "    num_heads = 3\n",
        "    mlp_ratio = 4.0\n",
        "    drop_rate = 0.0\n",
        "    attn_drop = 0.0\n",
        "    mixup_alpha = 0.8\n",
        "    use_cutmix = True\n",
        "    save_path = \"/content/vit_cifar_checkpt.pth\"\n",
        "    grad_clip = 1.0\n",
        "cfg = CFG()\n"
      ],
      "metadata": {
        "id": "brmZJ_3lMe7s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: seed\n",
        "def seed_all(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "seed_all(cfg.seed)\n"
      ],
      "metadata": {
        "id": "QfV1AY_4Mkc7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: dataloaders (RandAugment optional)\n",
        "from torchvision.transforms import AutoAugmentPolicy\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomCrop(cfg.img_size, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # optional: stronger augmentations (RandAugment)\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914,0.4822,0.4465),(0.247,0.243,0.261))\n",
        "])\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914,0.4822,0.4465),(0.247,0.243,0.261))\n",
        "])\n",
        "\n",
        "train_ds = datasets.CIFAR10(root=cfg.data_dir, train=True, download=True, transform=train_transforms)\n",
        "test_ds  = datasets.CIFAR10(root=cfg.data_dir, train=False, download=True, transform=test_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5v6II8xhMnPf",
        "outputId": "ac583790-c2c4-41cf-9b9d-6dc1feb067b9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:38<00:00, 4.38MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Mixup/CutMix helpers\n",
        "def rand_bbox(size, lam):\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = np.int(W * cut_rat)\n",
        "    cut_h = np.int(H * cut_rat)\n",
        "\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    return bbx1, bby1, bbx2, bby2\n"
      ],
      "metadata": {
        "id": "HdB20ZsXMqO1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Vision Transformer (simple, clean)\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=192):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        assert img_size % patch_size == 0, \"img_size must be divisible by patch_size\"\n",
        "        self.grid_size = img_size // patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: B,C,H,W\n",
        "        x = self.proj(x)           # B,embed, H/patch, W/patch\n",
        "        x = x.flatten(2).transpose(1,2)  # B, num_patches, embed_dim\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, dropout=attn_drop, batch_first=True)\n",
        "        self.drop_path = nn.Identity()\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=int(dim*cfg.mlp_ratio), drop=proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: B, N, D\n",
        "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, num_classes=10,\n",
        "                 embed_dim=192, depth=12, num_heads=3, drop_rate=0.):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches+1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        # transformer blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            AttentionBlock(dim=embed_dim, num_heads=num_heads, attn_drop=cfg.attn_drop, proj_drop=cfg.drop_rate)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # classifier\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        # init\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "            if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)           # B, N, D\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)  # B,1,D\n",
        "        x = torch.cat((cls_tokens, x), dim=1)          # B, N+1, D\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)\n",
        "        cls = x[:,0]\n",
        "        out = self.head(cls)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "TvnFvwm1MsvI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: instantiate\n",
        "model = VisionTransformer(\n",
        "    img_size=cfg.img_size,\n",
        "    patch_size=cfg.patch_size,\n",
        "    in_chans=cfg.in_chans,\n",
        "    num_classes=cfg.num_classes,\n",
        "    embed_dim=cfg.embed_dim,\n",
        "    depth=cfg.depth,\n",
        "    num_heads=cfg.num_heads,\n",
        "    drop_rate=cfg.drop_rate\n",
        ").to(cfg.device)\n",
        "\n",
        "# Loss with label smoothing\n",
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "    def forward(self, preds, target):\n",
        "        # preds: B, C\n",
        "        log_probs = F.log_softmax(preds, dim=-1)\n",
        "        nll = -log_probs.gather(dim=-1, index=target.unsqueeze(1)).squeeze(1)\n",
        "        smooth_loss = -log_probs.mean(dim=-1)\n",
        "        return ((1.0 - self.smoothing) * nll + self.smoothing * smooth_loss).mean()\n",
        "\n",
        "criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "# cosine annealing with warmup via lambda\n",
        "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=5*len(train_loader), num_training_steps=cfg.epochs*len(train_loader))\n"
      ],
      "metadata": {
        "id": "XL9bnzZmMs9C"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: train & eval\n",
        "def train_one_epoch(epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "    for step, (x,y) in pbar:\n",
        "        x = x.to(cfg.device); y = y.to(cfg.device)\n",
        "        # optional MixUp / CutMix could be applied here\n",
        "        preds = model(x)\n",
        "        loss = criterion(preds, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        if cfg.grad_clip:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.grad_clip)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        running_loss += loss.item()*x.size(0)\n",
        "        _, predicted = preds.max(1)\n",
        "        total += y.size(0)\n",
        "        correct += predicted.eq(y).sum().item()\n",
        "        pbar.set_description(f\"Epoch {epoch} Loss {(running_loss/total):.4f} Acc {100.*correct/total:.2f}\")\n",
        "    return running_loss/total, 100.*correct/total\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate():\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for x,y in test_loader:\n",
        "        x = x.to(cfg.device); y = y.to(cfg.device)\n",
        "        preds = model(x)\n",
        "        loss = criterion(preds, y)\n",
        "        running_loss += loss.item()*x.size(0)\n",
        "        _, predicted = preds.max(1)\n",
        "        total += y.size(0)\n",
        "        correct += predicted.eq(y).sum().item()\n",
        "    return running_loss/total, 100.*correct/total\n"
      ],
      "metadata": {
        "id": "P1fhDj7zMtLh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: run training\n",
        "best_acc = 0.0\n",
        "history = {\"train_loss\":[], \"train_acc\":[], \"val_loss\":[], \"val_acc\":[]}\n",
        "for epoch in range(1, cfg.epochs+1):\n",
        "    t0 = time.time()\n",
        "    tr_loss, tr_acc = train_one_epoch(epoch)\n",
        "    val_loss, val_acc = evaluate()\n",
        "    history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc)\n",
        "    history[\"val_loss\"].append(val_loss); history[\"val_acc\"].append(val_acc)\n",
        "    print(f\"Epoch {epoch} finished in {time.time()-t0:.1f}s | Train Acc {tr_acc:.2f} | Val Acc {val_acc:.2f}\")\n",
        "    # save best\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save({\n",
        "            \"model_state\": model.state_dict(),\n",
        "            \"cfg\": cfg.__dict__,\n",
        "            \"epoch\": epoch,\n",
        "            \"best_acc\": best_acc\n",
        "        }, cfg.save_path)\n",
        "        print(\"Saved best model:\", cfg.save_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6l1Z2LAGM2cw",
        "outputId": "e0497824-e3f3-4a90-9a9e-74b319c3aed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Epoch 1 Loss 2.0138 Acc 28.22: 100%|██████████| 196/196 [42:14<00:00, 12.93s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 finished in 2696.6s | Train Acc 28.22 | Val Acc 34.24\n",
            "Saved best model: /content/vit_cifar_checkpt.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Loss 1.8714 Acc 35.74:   1%|          | 2/196 [00:44<1:08:25, 21.16s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: final eval & quick plot\n",
        "val_loss, val_acc = evaluate()\n",
        "print(\"Final Test Acc:\", val_acc)\n",
        "# Optional: save history as numpy\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history[\"train_acc\"], label=\"train\")\n",
        "plt.plot(history[\"val_acc\"], label=\"val\")\n",
        "plt.xlabel(\"epoch\"); plt.ylabel(\"accuracy\"); plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "V2rFUwrIWZhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fySXd7VxM8qO"
      }
    }
  ]
}